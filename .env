# LLM Configuration
LLM_PROVIDER=openai  # or 'litellm', 'anthropic', 'vertex_ai', etc.

LITELLM_MODEL=vertex_ai/gemini-2.5-flash
LITELLM_BASE_URL=https://llmproxy.ai.orange
LITELLM_API_KEY=sk-AB_vQvlesxFayTUb_ZjvBg
# sk-AB_vQvlesxFayTUb_ZjvBg
LLM_MAX_TOKENS=16000
LLM_TEMPERATURE=0.2

# Vector Store Configuration
EMBEDDING_MODEL=vertex_ai/text-multilingual-embedding-002
VECTOR_INDEX_PATH=knowledge_base_flat.index
VECTOR_METADATA_PATH=knowledge_base_metadata.json

# Database Configuration
ENCRYPTION_SECRET=vGnHbXjAARLw5W9hdLNi4Lrqwf4x8C_zmiIfpA4nF_g=
DATABASE_PATH=feedback.db
SESSION_DB_PATH=session_history.db

# Server Configuration
SERVER_HOST=127.0.0.1
SERVER_PORT=5000
GRADIO_SHARE=false

# Vertex AI Configuration

VERTEX_PROJECT=sbx-31371-7ph6rv7dpsuqjtjp27j1
VERTEX_LOCATION=europe-west1
VERTEX_CREDENTIALS_PATH=vertex_service_key.json

# Cache Configuration
MAX_CACHE_SIZE=500
MAX_CHUNK_SIZE=50000

# Application Settings
MAX_SESSIONS=10
DEFAULT_LANGUAGE=English

# Large file processing settings

MAX_FILE_SIZE_CHARS=1000000
CHUNK_SIZE_CHARS=50000
CHUNK_OVERLAP_CHARS=5000
MAX_CHUNKS_PER_FILE=1000
PARALLEL_CHUNK_PROCESSING=true

# Streaming and Completion Settings
ENABLE_STREAMING=true
COMPLETE_RESPONSE_COLLECTION=true
LLM_MAX_RETRIES=3
STREAM_BUFFER_SIZE=1024000  # Large buffer for streaming
